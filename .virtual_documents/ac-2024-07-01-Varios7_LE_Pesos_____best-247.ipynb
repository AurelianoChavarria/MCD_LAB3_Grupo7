














import pandas as pd

def calcular_total_forecast_error(actual, forecast):
    """
    Calcula el Total Forecast Error dado un DataFrame con ventas reales y pronosticadas.

    Parámetros:
    actual (pd.Series): Serie con las ventas reales.
    forecast (pd.Series): Serie con las ventas pronosticadas.

    Retorna:
    float: El Total Forecast Error.
    """
    # Calcular el error absoluto
    abs_error = abs(actual - forecast)
    
    # Calcular el Total Forecast Error
    total_forecast_error = abs_error.sum() / actual.sum()
    
    print("\n\n-----------------------------------------------------------------------------")
    print("-----------------------------------------------------------------------------")
    print(f'    >>>>>>>>>>>>       Total Forecast Error: {total_forecast_error:.2%}     <<<<<<<<<<<<<<<<<<')
    print("-----------------------------------------------------------------------------")
    print("-----------------------------------------------------------------------------\n\n")

    
    return total_forecast_error

# Ejemplo de uso
# Crear un DataFrame de ejemplo
data = {
    'product_id': [1, 2, 3, 4, 5],
    'actual_sales': [100, 150, 200, 250, 300],
    'forecast_sales': [110, 145, 190, 260, 310]
}
df = pd.DataFrame(data)

# Calcular el Total Forecast Error
tfe = calcular_total_forecast_error(df['actual_sales'], df['forecast_sales'])
#print("\n\n-----------------------------------------------------------------------------")
#print("-----------------------------------------------------------------------------")
#print(f'    >>>>>>>>>>>>       Total Forecast Error: {tfe:.2%}     <<<<<<<<<<<<<<<<<<')
#print("-----------------------------------------------------------------------------")
#print("-----------------------------------------------------------------------------\n\n")







# Funciones para calcular el tiempo de ejecucion del script completo
from datetime import datetime
def registrar_tiempo():
    ahora = datetime.now()
    ahora_str = ahora.strftime("%Y-%m-%d %H:%M:%S")
    return ahora, ahora_str

def calcular_tiempo_transcurrido(inicio, fin):
    return (fin - inicio).total_seconds()

# Registrar el tiempo de inicio al inicio del script
inicio, inicio_str = registrar_tiempo()






# --------------------------------------
# Funcion nombre_file(sfx)
#   - Genera el nombre del archivo .csv con prefijo datetime YYYY-MM-DD
from datetime import datetime
def nombre_file(sfx):
    # Obtener la fecha y hora actual en el formato requerido
    current_time = datetime.now().strftime('%Y%m%d-%H%M-')

    # Path to output dir
    output_dir = '../666_Kaggle/Entregas/'    
    return(output_dir+current_time+sfx+'.csv')


# Indicar el nombre de la prueba
suffix_name = 'MongoAurelio' 

file_to_kaggle = nombre_file(suffix_name)
print(file_to_kaggle)


# ------------------------------------------------
# Esto va al final para escribir el archivo final

# Agrego el tfe2 al suffix del archivo
#   Calculo del total forecast error
#   tfe2 =  calcular_total_forecast_error(all_forecasts['y'], all_forecasts['yhat1'])
#    print(f'Total Forecast Error: {tfe2:.2%}')

tfe2 = 0.123456789
str_tfe2 = "_tfe2_" + str(round(tfe2, 4)) 

# Suffijo general para las dos salidas de archivos
#  Solo cambiar este valor
suffix_general = 'Sufijo-General'  + str_tfe2

# Usar la función nombre_file para asignar el nombre del archivo de salida para kaggle
suffix_to_kaagle_name = suffix_general
file_to_kaggle = nombre_file(suffix_to_kaagle_name)
# Colocar el nombre del df apropiado
#df_aGuardarEnDisco.to_csv(file_to_kaggle, index=False)

#all_forecasts.to_csv(file_to_kaggle+'all', index=False)
print(f'Predicciones ajustadas guardadas en {file_to_kaggle}')

# Fin
# ------------------------------------------------







import pandas as pd

ventas_LTSM_path = '../66_Datos/sell-z-780-all-LTSM.csv'
df_ventas = pd.read_csv(ventas_LTSM_path)

# Convertir la columna 'periodo' a tipo datetime
df_ventas['periodo'] = pd.to_datetime(df_ventas['periodo'], format='%Y-%m-%d')

# Formatear la fecha según el formato deseado
print(df_ventas.info())

df_ventas.head(2)


# Asumiendo que 'df_ventas' es tu DataFrame y la columna 'periodo' está en formato yyyymm
# Filtramos los datos para excluir noviembre y diciembre de 2019


#df_ventas = df_ventas[~df_ventas['periodo'].isin(['2017-01-01','2017-02-01','2017-03-01','2017-04-01','2017-05-01','2017-06-01','2017-07-01','2017-08-01','2017-09-01','2017-10-01','2017-11-01','2017-12-01','2019-08-01' ])]
#df_ventas = df_ventas[df_ventas['periodo'].isin(['2019-01-01','2019-02-01','2019-03-01','2019-04-01','2019-05-01','2019-06-01','2019-07-01','2019-08-01','2019-09-01','2019-11-01','2019-12-01'])]
# Suponiendo que df_ventas ya está definido
df_ventas = df_ventas.reset_index(drop=True)

# Verifica que los periodos han sido eliminados



df_ventas.info()


aa = df_ventas.loc[df_ventas['product_id']== 21276]
#aa.to_csv('../66_Datos/21276.csv', index=False)
aa.head(2)














# Con Normalizacion StandarScaler() y Transformación Logarítmica

import pandas as pd

ventas_LTSM_path = '../66_Datos/sell-z-780-all-LTSM.csv'
df_ventas = pd.read_csv(ventas_LTSM_path)

# Convertir la columna 'periodo' a tipo datetime
df_ventas['periodo'] = pd.to_datetime(df_ventas['periodo'], format='%Y-%m-%d')

# Formatear la fecha según el formato deseado
#print(df_ventas.info())

df_ventas.head(2)

import pandas as pd
import numpy as np
import random
import tensorflow as tf
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import legacy
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

# Fijar la semilla para reproducibilidad
SEED = 52
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

def process_and_train_model(df_ventas, seq_length=12, epochs=100, batch_size=32, learning_rate=0.001, patience=20, verbose=0):
    # Preparación de datos
    df_full = df_ventas[['periodo', 'product_id', 'cat1', 'tn']].copy()
    
    # Identificar los valores cero antes de reemplazar
    valores_cero_antes = df_full[df_full['tn'] == 0].copy()

    # Calcular la media de 'tn' por 'product_id' y reemplazar los valores cero por esta media
    mean_tn_by_product = df_full[df_full['tn'] != 0].groupby('product_id')['tn'].mean()
    df_full.loc[df_full['tn'] == 0, 'tn'] = df_full['product_id'].map(mean_tn_by_product)

    # Reemplazar NaN con la media global de tn
    global_mean_tn = df_full['tn'].mean()
    df_full['tn'].fillna(global_mean_tn, inplace=True)

    # Identificar los valores que fueron reemplazados
    valores_cero_despues = df_full.loc[valores_cero_antes.index]
    valores_reemplazados = valores_cero_despues[valores_cero_despues['tn'] != 0]

    # Imprimir los valores que fueron reemplazados
    print(f"Valores en 0 antes del reemplazo:\n{valores_cero_antes.head(2)}")
    print(f"Valores después del reemplazo:\n{valores_reemplazados.head(2)}")
    
    # Aplicar la transformación logarítmica
    df_full['tn'] = np.log1p(df_full['tn'])
    
    # Estandarización de datos
    scaler = StandardScaler()
    df_full['tn'] = scaler.fit_transform(df_full[['tn']])
    
    # Label Encoding para cat1
    label_encoder = LabelEncoder()
    df_full['cat1'] = label_encoder.fit_transform(df_full['cat1'])

    def create_sequences(data, seq_length):
        sequences = []
        labels = []
        for i in range(len(data) - seq_length):
            seq_data = data.iloc[i:i + seq_length].values
            sequences.append(seq_data)
            labels.append(data.iloc[i + seq_length]['tn'])
        return np.array(sequences), np.array(labels)

    product_sequences = {}
    for product_id in df_full['product_id'].unique():
        product_data = df_full[df_full['product_id'] == product_id].drop(columns=['product_id', 'periodo'])
        sequences, labels = create_sequences(product_data, seq_length)
        product_sequences[product_id] = (sequences, labels)

    # Preparar los datos de entrenamiento
    X = np.concatenate([product_sequences[pid][0] for pid in product_sequences])
    y = np.concatenate([product_sequences[pid][1] for pid in product_sequences])
    product_ids = np.concatenate([[pid]*len(product_sequences[pid][1]) for pid in product_sequences])
    
    # Redimensionar los datos para que sean compatibles con LSTM
    X = X.reshape(X.shape[0], X.shape[1], X.shape[2])

    # Utilizar TimeSeriesSplit para dividir los datos
    tscv = TimeSeriesSplit(n_splits=5)
    
    for train_index, val_index in tscv.split(X):
        X_train, X_val = X[train_index], X[val_index]
        y_train, y_val = y[train_index], y[val_index]
        
        model = Sequential([
            LSTM(50, return_sequences=True, input_shape=(seq_length, X.shape[2])),  # Primera capa LSTM
            LSTM(50, return_sequences=False),  # Segunda capa LSTM
            Dropout(0.2),  # Mantener el dropout para regularización
            Dense(25, activation='relu'),  # Reducir el tamaño de la capa densa
            Dense(1)  # Capa de salida
        ])
        
        optimizer = legacy.Adam(learning_rate=learning_rate)
        model.compile(optimizer=optimizer, loss='mean_squared_error')

        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)

        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)

        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[early_stopping, reduce_lr], verbose=verbose)

        X_pred = np.array([product_sequences[pid][0][-1] for pid in product_sequences])
        predictions = model.predict(X_pred)
        predictions = scaler.inverse_transform(predictions)
        
        # Invertir la transformación logarítmica
        predictions = np.expm1(predictions)

        results = pd.DataFrame({
            'product_id': df_full['product_id'].unique(),
            'tn_pred': predictions.flatten()
        })

        y_pred_val = model.predict(X_val)

        y_val = y_val.reshape(-1, 1)
        y_pred_val = y_pred_val.reshape(-1, 1)
        
        y_pred_val = scaler.inverse_transform(y_pred_val)
        y_val = scaler.inverse_transform(y_val)

        # Invertir la transformación logarítmica para los valores de validación y predicción
        y_pred_val = np.expm1(y_pred_val)
        y_val = np.expm1(y_val)

        plt.figure(figsize=(10,6))
        plt.plot(y_val, label='Valor Real')
        plt.plot(y_pred_val, label='Predicción')
        plt.legend()
        plt.title('Predicciones vs Valores Reales')
        plt.show()

        # Calcular el error absoluto para cada predicción
        error_abs = np.abs(y_val - y_pred_val)

        # Crear un DataFrame para almacenar los resultados con errores
        df_errors = pd.DataFrame({
            'product_id': product_ids[val_index],  # Asegúrate de que esto contiene los IDs de los productos correspondientes
            'y_val': y_val.flatten(),
            'y_pred_val': y_pred_val.flatten(),
            'error_abs': error_abs.flatten()
        })

        # Ordenar por error absoluto en orden descendente para identificar los mayores errores
        df_errors_sorted = df_errors.sort_values(by='error_abs', ascending=False)

        # Identificar los top 10 productos con mayores errores
        top_errors = df_errors_sorted.head(10)

        print("Top 10 productos con mayores errores:")
        print(top_errors)

        tfe = np.sum(np.abs(y_val - y_pred_val)) / np.sum(np.abs(y_val))

        mse = mean_squared_error(y_val, y_pred_val)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_val, y_pred_val)

        print("\n-------------------------------------------------------")
        print("Cant de productos a predecir: ", df_ventas.product_id.nunique())
        print("\n")
        print(f'TFE: {tfe}')
        print(f'MSE: {mse}')
        print(f'RMSE: {rmse}')
        print(f'MAE: {mae}')

    return results, df_errors_sorted

# Ejemplo de uso de la función

# kaggle 253
#results, df_errors_sorted = process_and_train_model(df_ventas, seq_length=12, epochs=100, batch_size=32, learning_rate=0.0001, patience=10, verbose=0)
results, df_errors_sorted = process_and_train_model(df_ventas, seq_length=12, epochs=100, batch_size=32, learning_rate=0.0001, patience=10, verbose=0)

# Guardar resultados
results_toCsv = results.copy()
results_toCsv = results_toCsv.rename(columns={'tn_pred': 'tn'})

tfe2 = 0
str_tfe2 = "_tfe2_" + str(round(tfe2, 4)) 

# Guardar el archivo final
suffix_general = 'LSTM_LE_varios7_ene' + "_tfe2_" + str(round(0, 4))
file_to_kaggle = nombre_file(suffix_general)
results.to_csv(file_to_kaggle, index=False)

# Calcular el siguiente período (suponiendo que los periodos son mensuales)
df_ventas['periodo'] = pd.to_datetime(df_ventas['periodo'], format='%Y%m')
ultimo_periodo = df_ventas['periodo'].max()
siguiente_periodo = ultimo_periodo + pd.DateOffset(months=1)

# Actualizar las 'tn' del siguiente período con las 'tn' de results
df_ventas['periodo'] = df_ventas['periodo'].dt.strftime('%Y%m')
results_toCsv['periodo'] = siguiente_periodo.strftime('%Y%m')
df_ventas2 = pd.concat([df_ventas, results_toCsv], ignore_index=True)

# Vuelvo a convertir a 'periodo' a tipo datetime
#      Convertir la columna 'periodo' a datetime con el formato %Y%m
df_ventas['periodo'] = pd.to_datetime(df_ventas['periodo'], format='%Y%m')
df_ventas2['periodo'] = pd.to_datetime(df_ventas2['periodo'], format='%Y%m')
#      Formatear la columna 'periodo' a YYYY-MM-DD, estableciendo el día como 01
df_ventas['periodo'] = df_ventas['periodo'].dt.strftime('%Y-%m-%d')
df_ventas2['periodo'] = df_ventas2['periodo'].dt.strftime('%Y-%m-%d')


# Verificar el resultado
print(df_ventas2.tail(2))

# Entrenar el modelo con datos hasta el último período actualizado
results2, df_errors_sorted2 = process_and_train_model(df_ventas2, seq_length=12, epochs=100, batch_size=32, learning_rate=0.0001, patience=10, verbose=0)
results2 = results2.rename(columns=({'tn_pred': 'tn'}))

# Verificar el resultado
print(results2.head(2))

# Guardar el archivo final
suffix_general = 'LSTM_LE_varios7_feb' + "_tfe2_" + str(round(0, 4))
file_to_kaggle = nombre_file(suffix_general)
results2.to_csv(file_to_kaggle, index=False)

print(f'Predicciones ajustadas guardadas en {file_to_kaggle}')








