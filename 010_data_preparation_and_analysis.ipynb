{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "K0U2xhQlhuQy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<table style=\"text-align: left; width: 100%;\" border=\"0\"\n",
    " cellpadding=\"0\" cellspacing=\"0\">\n",
    "\n",
    "  <tbody>\n",
    "    <tr align=\"center\" style=\"height: 1px; background-color: rgb(0, 0, 0);\">\n",
    "      <td><big><big><big><big>Grupo-7</big></big></big></td>\n",
    "      <td><big><big><big><big><span style=\"font-family: Calibri; color: white; font-weight: bold;\">Laboratorio III</span></big></big></big></big></td>\n",
    "      <td><img src=\"https://i.imgur.com/YOQky86.png\" title=\"source: imgur.com\" style=\"width: 250px; height: auto;\" /></td>\n",
    "    </tr>\n",
    "    <tr align=\"center\">\n",
    "      <td colspan=\"3\" rowspan=\"1\"\n",
    " style=\"height: 1px; background-color: rgb(68, 68, 100);\"></td>\n",
    "    </tr>\n",
    "    <tr align=\"center\">\n",
    "      <td colspan=\"3\" rowspan=\"1\"><big><big><big><big><span\n",
    " style=\"font-family: Calibri;\">Data Preparation & Analysis</span></big></big></big></big><br>\n",
    "      </td></tr>\n",
    "    <tr align=\"center\">       \n",
    "      <td colspan=\"3\" rowspan=\"1\" style=\"height: 1px; background-color: rgb(68, 68, 100);\"></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "<span style=\"font-family: Calibri; font-weight: bold; \">Autores:</span>\n",
    "<br style=\"font-family: Calibri; font-style: italic;\">\n",
    "<span style=\"font-family: Calibri; font-style: italic;\">\n",
    "- Aureliano Chavarria\n",
    "- Gastón Larregui\n",
    "- Patricia Nuñez\n",
    "</span>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"footer\">&copy; 2024</div>\n",
    "\n",
    "\n",
    "jupyter nbconvert --to script xxx.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Esta notebook fue está diseñada para llevar a cabo diversas tareas esenciales en el proceso de preparación y análisis de datos de ventas. Su contenido abarca una serie de funciones y procedimientos que facilitan la manipulación, evaluación y visualización de los datos. A continuación, se describen brevemente los componentes principales del script:\n",
    "\n",
    "1. **Conversión y Formateo de Fechas**:\n",
    "   - Se convierte el formato de las fechas en el DataFrame `df_ventas` al tipo `datetime` de pandas, lo cual es crucial para realizar análisis temporales y manipular los datos de manera eficiente.\n",
    "\n",
    "2. **Cálculo del Error Total de Pronóstico**:\n",
    "   - Se incluye una función `calcular_total_forecast_error` que calcula el error total de pronóstico (Total Forecast Error) comparando las ventas reales con las pronosticadas. Esta métrica es fundamental para evaluar la precisión de los modelos predictivos desarrollados.\n",
    "\n",
    "3. **Análisis y Filtrado de Productos**:\n",
    "   - Se realiza el filtrado de productos sin categoría (`sinCatego`) y la identificación de productos que faltan en la descripción de productos (`df_productos_descripcion`). Para mantener la integridad de los datos, se crean DataFrames para estos productos faltantes y se completan con valores predeterminados.\n",
    "\n",
    "4. **Generación de Archivos CSV para PostgreSQL**:\n",
    "   - Se generan archivos CSV (`sell-in-all-to-postgres.csv` y `tb_productos_descripcion_ac.csv`) que están listos para ser cargados en una base de datos PostgreSQL. Esta etapa es esencial para la preparación de datos y su posterior análisis utilizando herramientas como Grafana.\n",
    "\n",
    "5. **Visualización de Datos Específicos**:\n",
    "   - Se incluyen ejemplos para visualizar subconjuntos específicos de datos, permitiendo un análisis más detallado de casos particulares, como productos con un `product_id` específico.\n",
    "\n",
    "Este script es una herramienta versátil que automatiza y simplifica varias etapas del proceso de análisis de datos, desde la preparación inicial hasta la evaluación y generación de archivos para su posterior explotación en bases de datos y plataformas de visualización.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Prediccion Feb 2020: Lectura Completa y Rapida de ventas\n",
    "## Dataset sell-z-780-all-LTSM.csv\n",
    "\n",
    "**Nota:** Este `df_ventas` genera `sell-z-780-all-LTSM.csv` y solo contiene los datos de los 780 productos a predecir.\n",
    "\n",
    "Luego de ejecutar tenemos el `df_ventas` con: \n",
    "- Todas las ventas de los 780 productos a predecir\n",
    "- Matriz de 780 productos * 36 periodos = 28080 (se completaron con 0 los periodos faltantes para cada producto)\n",
    "- Columnas con la clasificacion de productos: cat1, cat2, cat3, brand, sku_size, descripcion\n",
    "- Columna en780 (deberia estar todo en 1 ya que indica que es producto a predecir para febrero 2020)\n",
    "- Columna cant_periodos: contiene para cada producto la cantidad de periodos que tuvo ventas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de ventas y productos a predecir\n",
    "#sell-z-780-groupBy-Period-ProdId.csv\n",
    "#ventas_file_path = '../66_Datos/sell-in.txt'\n",
    "ventas_file_path = '../66_Datos/sell-z-780-groupBy-Period-ProdId.csv'\n",
    "productos_file_path = '../66_Datos/productos_a_predecir.txt'\n",
    "productos_descripcion = '../66_Datos/tb_productos_descripcion.txt'\n",
    "\n",
    "\n",
    "# tb_productos_descripcion.txt es la ultima tabla (∫2024/06/15) que presento con las caracteristicas y categoria de cada producto\n",
    "df_productos_descripcion = pd.read_csv(productos_descripcion, sep='\\t')\n",
    "df_productos_descripcion = df_productos_descripcion[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion']]\n",
    "df_productos_descripcion = df_productos_descripcion.reset_index(drop=True)\n",
    "# Nota: no encuentro duplicados en productos_descripcion. De todas formas armo la sentencia de eliminarlos ya que pueden aparecer\n",
    "#       futuras versiones del archivo\n",
    "# Eliminar los duplicados en la columna 'product_id', manteniendo solo la primera aparición\n",
    "df_productos_descripcion.drop_duplicates(subset='product_id', keep='first', inplace=True)\n",
    "\n",
    "#df_ventas_orig = pd.read_csv(ventas_file_path, sep='\\t')\n",
    "df_ventas_orig = pd.read_csv(ventas_file_path)\n",
    "\n",
    "# Borro columna customer_id \n",
    "#   Esta columna viene del archivo csv que agrupo valores por product_id y periodo. No sabemos si customer_id es un valor totalizado.\n",
    "#   En principio no lo necesito\n",
    "df_ventas = df_ventas_orig.copy()\n",
    "del df_ventas['customer_id']\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Agregar Periodos faltantes con valor = 0\n",
    "#    LSTM necesita toda la serie temporal en la que los productos fueron vendidos para poder predecir a futuro, incluso\n",
    "#    en meses dde no hubieron ventas. Para estos meses se completa el periodo, product_id y valores a cero para tn (y demas valores tipo numericos)\n",
    "\n",
    "# Primero convertimos la columna 'periodo' en datetime si no está en ese formato\n",
    "df_ventas['periodo'] = pd.to_datetime(df_ventas['periodo'], format='%Y%m')\n",
    "\n",
    "meses = df_ventas.periodo.unique()\n",
    "prod = df_ventas.product_id.unique()\n",
    "\n",
    "all_combinations = pd.MultiIndex.from_product([prod, meses], names=['product_id', 'periodo']).to_frame(index=False)\n",
    "\n",
    "# Unir los datos originales con el DataFrame completo para asegurar que cada producto tenga todos los periodos\n",
    "df_full = pd.merge(all_combinations, df_ventas, on=['product_id', 'periodo'], how='left')\n",
    "\n",
    "# Rellenar los valores faltantes con ceros\n",
    "df_full['tn'] = df_full['tn'].fillna(0)\n",
    "\n",
    "df_ventas = df_full.copy()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Agregar la col cant_periodos\n",
    "#    Cant periodos indica cuantos meses hubo ventas de un product_id\n",
    "# Filtrar el DataFrame para excluir los períodos donde 'tn' es cero\n",
    "df_ventas_filtrado = df_ventas[df_ventas['tn'] != 0]\n",
    "\n",
    "# Agrupar por 'product_id' y contar los períodos únicos en el DataFrame filtrado\n",
    "cant_periodos = df_ventas_filtrado.groupby('product_id')['periodo'].nunique()\n",
    "\n",
    "# Agregamos esta información al DataFrame original\n",
    "df_ventas = df_ventas.merge(cant_periodos.rename('cant_periodos'), on='product_id', how='left')\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Agregamos la descripcion de cada producto (cat1, cat2, cat3, etc)\n",
    "#   Esto podria ser util en caso de querer incorporar nuevas variables al modelo\n",
    "df_ventas = pd.merge(df_ventas, df_productos_descripcion, on=['product_id'], how='left')\n",
    "\n",
    "# Contar la cantidad de NaN en cada columna\n",
    "cantidad_nans_por_columna = df_ventas.isna().sum()\n",
    "print('\\n--------------------------------------')\n",
    "print('Cantidad de NaN x Columna:')\n",
    "print(cantidad_nans_por_columna)\n",
    "\n",
    "# Rellenar los valores faltantes con ceros\n",
    "df_ventas = df_ventas.fillna(0)\n",
    "\n",
    "print('\\n--------------------------------------')\n",
    "print('Completando a 0 los NaN')\n",
    "\n",
    "\n",
    "\n",
    "# Contar la cantidad de NaN en cada columna\n",
    "cantidad_nans_por_columna = df_ventas.isna().sum()\n",
    "print('\\n--------------------------------------')\n",
    "print('Cantidad de NaN x Columna:')\n",
    "print(cantidad_nans_por_columna)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n--------------------------------------\\n')\n",
    "print('Generando archivo final: ../66_Datos/sell-z-780-all-LTSM.csv')\n",
    "\n",
    "# En caso de generar el archivo a csv para uso futuro\n",
    "#df_ventas.to_csv('../66_Datos/sell-z-780-all-LTSM.csv', index=False)\n",
    "df_ventas.round(1).to_excel('../66_Datos/sell-z-780-all-LTSM.xlsx', index=False)\n",
    "\n",
    "print('\\n--------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecutura rapida de Dataset sell-z-780-all-LTSM.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ventas_LTSM_path = '../66_Datos/sell-z-780-all-LTSM.csv'\n",
    "df_ventas = pd.read_csv(ventas_LTSM_path)\n",
    "\n",
    "# Convertir la columna 'periodo' a tipo datetime\n",
    "df_ventas['periodo'] = pd.to_datetime(df_ventas['periodo'], format='%Y-%m-%d')\n",
    "\n",
    "# Formatear la fecha según el formato deseado\n",
    "print(df_ventas.info())\n",
    "\n",
    "df_ventas.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Formula Total Forecast Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def calcular_total_forecast_error(actual, forecast):\n",
    "    \"\"\"\n",
    "    Calcula el Total Forecast Error dado un DataFrame con ventas reales y pronosticadas.\n",
    "\n",
    "    Parámetros:\n",
    "    actual (pd.Series): Serie con las ventas reales.\n",
    "    forecast (pd.Series): Serie con las ventas pronosticadas.\n",
    "\n",
    "    Retorna:\n",
    "    float: El Total Forecast Error.\n",
    "    \"\"\"\n",
    "    # Calcular el error absoluto\n",
    "    abs_error = abs(actual - forecast)\n",
    "    \n",
    "    # Calcular el Total Forecast Error\n",
    "    total_forecast_error = abs_error.sum() / actual.sum()\n",
    "    \n",
    "    print(\"\\n\\n-----------------------------------------------------------------------------\")\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    print(f'    >>>>>>>>>>>>       Total Forecast Error: {total_forecast_error:.2%}     <<<<<<<<<<<<<<<<<<')\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    print(\"-----------------------------------------------------------------------------\\n\\n\")\n",
    "\n",
    "    \n",
    "    return total_forecast_error\n",
    "\n",
    "# Ejemplo de uso\n",
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'product_id': [1, 2, 3, 4, 5],\n",
    "    'actual_sales': [100, 150, 200, 250, 300],\n",
    "    'forecast_sales': [110, 145, 190, 260, 310]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calcular el Total Forecast Error\n",
    "tfe = calcular_total_forecast_error(df['actual_sales'], df['forecast_sales'])\n",
    "#print(\"\\n\\n-----------------------------------------------------------------------------\")\n",
    "#print(\"-----------------------------------------------------------------------------\")\n",
    "#print(f'    >>>>>>>>>>>>       Total Forecast Error: {tfe:.2%}     <<<<<<<<<<<<<<<<<<')\n",
    "#print(\"-----------------------------------------------------------------------------\")\n",
    "#print(\"-----------------------------------------------------------------------------\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Total Forecast Error} = \\frac{\\sum_{\\text{sku}} |\\text{Actual Sales} - \\text{Forecast Sales}|}{\\sum_{\\text{sku}} \\text{Actual Sales}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Funcion nombre_file(sfx)\n",
    "   - Genera el nombre del archivo .csv con prefijo datetime YYYY-MM-DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Funcion nombre_file(sfx)\n",
    "#   - Genera el nombre del archivo .csv con prefijo datetime YYYY-MM-DD\n",
    "from datetime import datetime\n",
    "def nombre_file(sfx):\n",
    "    # Obtener la fecha y hora actual en el formato requerido\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d-%H%M-')\n",
    "\n",
    "    # Path to output dir\n",
    "    output_dir = '../666_Kaggle/Entregas/'    \n",
    "    return(output_dir+current_time+sfx+'.csv')\n",
    "# Indicar el nombre de la prueba\n",
    "suffix_name = 'MongoAurelio' \n",
    "\n",
    "file_to_kaggle = nombre_file(suffix_name)\n",
    "print(file_to_kaggle)\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Esto va al final para escribir el archivo final\n",
    "\n",
    "# Agrego el tfe2 al suffix del archivo\n",
    "#   Calculo del total forecast error\n",
    "#   tfe2 =  calcular_total_forecast_error(all_forecasts['y'], all_forecasts['yhat1'])\n",
    "#    print(f'Total Forecast Error: {tfe2:.2%}')\n",
    "\n",
    "tfe2 = 0.123456789\n",
    "str_tfe2 = \"_tfe2_\" + str(round(tfe2, 4)) \n",
    "\n",
    "# Suffijo general para las dos salidas de archivos\n",
    "#  Solo cambiar este valor\n",
    "\n",
    "suffix_general = 'Sufijo-General'  + str_tfe2\n",
    "\n",
    "# Usar la función nombre_file para asignar el nombre del archivo de salida para kaggle\n",
    "suffix_to_kaagle_name = suffix_general\n",
    "file_to_kaggle = nombre_file(suffix_to_kaagle_name)\n",
    "# Colocar el nombre del df apropiado\n",
    "#df_aGuardarEnDisco.to_csv(file_to_kaggle, index=False)\n",
    "\n",
    "#all_forecasts.to_csv(file_to_kaggle+'all', index=False)\n",
    "print(f'Predicciones ajustadas guardadas en {file_to_kaggle}')\n",
    "\n",
    "# Fin\n",
    "# ------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------\n",
    "# Funcion nombre_file(sfx)\n",
    "#   - Genera el nombre del archivo .csv con prefijo datetime YYYY-MM-DD\n",
    "from datetime import datetime\n",
    "def nombre_file(sfx):\n",
    "    # Obtener la fecha y hora actual en el formato requerido\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d-%H%M-')\n",
    "\n",
    "    # Path to output dir\n",
    "    output_dir = '../666_Kaggle/Entregas/'    \n",
    "    return(output_dir+current_time+sfx+'.csv')\n",
    "\n",
    "# Indicar el nombre de la prueba\n",
    "\n",
    "\n",
    "# Agrego el tfe2 al suffix del archivo\n",
    "#   Calculo del total forecast error\n",
    "#   tfe2 =  calcular_total_forecast_error(all_forecasts['y'], all_forecasts['yhat1'])\n",
    "#    print(f'Total Forecast Error: {tfe2:.2%}')\n",
    "\n",
    "tfe2 = 0.123456789\n",
    "str_tfe2 = \"_tfe2_\" + str(round(tfe2, 4)) \n",
    "\n",
    "suffix_name = 'MongoAurelio' + str_tfe2\n",
    "file_to_kaggle = nombre_file(suffix_name)\n",
    "print(file_to_kaggle)\n",
    "# --------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#  Sell-in - Datos de Ventas \n",
    "\n",
    "### sell-in.txt\n",
    "\n",
    "El archivo de ventas contiene todos los datos de ventas facilitados por el docente. Es la tabla punto de partida para las ventas. \n",
    "\n",
    "Contiene las siguientes columnas:\n",
    "\n",
    "1. **periodo**: Representa el período de tiempo en formato `yyyymm` (año y mes).\n",
    "    - Minimum\t2017-01-01 00:00:00\n",
    "    - Maximum\t2019-12-01 00:00:00\n",
    "2. **customer_id**: Identificación única del cliente.\n",
    "3. **product_id**: Identificación única del producto.\n",
    "4. **plan_precios_cuidados**: Indicador binario (0 o 1) que señala si el producto está incluido en el plan de precios cuidados.\n",
    "5. **cust_request_qty**: Cantidad de producto solicitada por el cliente.\n",
    "6. **cust_request_tn**: Peso en toneladas solicitado por el cliente.\n",
    "7. **tn**: Peso en toneladas del producto vendido.\n",
    "\n",
    "Estas columnas proporcionan información detallada sobre las ventas realizadas en distintos períodos, especificando el cliente, el producto, y las cantidades en unidades y en peso.\n",
    "\n",
    "\n",
    "### sell-z-780-all-prodId.csv\n",
    "sell-z-780-all-prodId.csv es una vista de la tabla sell-in.txt con solamente los movimientos de los 780 productos que hay que predeicr (productos_a_predecir.txt).\n",
    "Las ventas de productos por peridos no estan agregadas. Esto es, existen para un mismo producto en un mismo perodo multiples entradas.  \n",
    "  EJ:\n",
    "\n",
    "| Periodo | product_id | tn   |\n",
    "|---------|------------|------|\n",
    "| 2018-01 | 20001      | 1360 |\n",
    "| 2018-01 | 20001      | 160  |\n",
    "| 2018-01 | 20055      | 3360 |\n",
    "\n",
    "### sell-z-780-groupBy-Period-ProdId.csv\n",
    "sell-z-780-groupBy-Period-ProdId.csv contiene los mismos datos que sell-z-780-all-prodId.csv pero agrupados por 'periodo' y 'product_id'. Es decir, para un producto que tuvo multiples ventas en un periodo tengo agrupadas las ventas para ese periodo.\n",
    "\n",
    "Ej. \n",
    "| Periodo | product_id | tn   |\n",
    "|---------|------------|------|\n",
    "| 2018-01 | 20001      | 1520 |\n",
    "| 2018-01 | 20055      | 3360 |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de ventas y productos a predecir\n",
    "ventas_file_path = '../66_Datos/sell-in.txt'\n",
    "productos_file_path = '../66_Datos/productos_a_predecir.txt'\n",
    "productos_descripcion = '../66_Datos/tb_productos_descripcion.txt'\n",
    "\n",
    "df_productos_descripcion = pd.read_csv(productos_descripcion, sep='\\t')\n",
    "df_productos_descripcion = df_productos_descripcion[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion']]\n",
    "df_productos_descripcion = df_productos_descripcion.reset_index(drop=True)\n",
    "# Nota: no encuentro duplicados en productos_descripcion. De todas formas armo la sentencia de eliminarlos ya que pueden aparecer\n",
    "#       futuras versiones del archivo\n",
    "# Eliminar los duplicados en la columna 'product_id', manteniendo solo la primera aparición\n",
    "df_productos_descripcion.drop_duplicates(subset='product_id', keep='first', inplace=True)\n",
    "\n",
    "df_ventas_orig = pd.read_csv(ventas_file_path, sep='\\t')\n",
    "\n",
    "df_ventas = pd.merge(df_ventas_orig, df_productos_descripcion, on=['product_id'], how='left')\n",
    "\n",
    "\n",
    "df_productos = pd.read_csv(productos_file_path, sep='\\t')\n",
    "\n",
    "\n",
    "# Filtrar las ventas para que solo incluya los productos a predecir\n",
    "productos_a_predecir = df_productos['product_id'].unique()\n",
    "sell_780_all = df_ventas[df_ventas['product_id'].isin(productos_a_predecir)]\n",
    "\n",
    "\n",
    "sell_780_groupBy_Periodo_ProdId = sell_780_all.groupby(['periodo', 'product_id'], as_index=False).sum()\n",
    "\n",
    "# Opcional: Guardar el DataFrame filtrado en un archivo CSV\n",
    "#df['ds'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df_ventas_toCsv = df_ventas\n",
    "df_ventas_toCsv['periodo'] = pd.to_datetime(df_ventas_toCsv['periodo'], format='%Y%m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar CSVs \n",
    "df_ventas_toCsv.to_csv('../66_Datos/sell-in.csv', index=False)\n",
    "sell_780_all.to_csv('../66_Datos/sell-z-780-all-prodId.csv', index=False)\n",
    "sell_780_groupBy_Periodo_ProdId.to_csv('../66_Datos/sell-z-780-groupBy-Period-ProdId.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_productos_descripcion = df_productos_descripcion[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion']]\n",
    "df_productos_descripcion = df_productos_descripcion.reset_index(drop=True)\n",
    "\n",
    "# Nota: no encuentro duplicados en productos_descripcion. De todas formas armo la sentencia de eliminarlos ya que pueden aparecer\n",
    "#       futuras versiones del archivo\n",
    "# Eliminar los duplicados en la columna 'product_id', manteniendo solo la primera aparición\n",
    "df_productos_descripcion.drop_duplicates(subset='product_id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.merge(df_ventas, df_productos_descripcion, on=['product_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV de las ventas de todos los 780 con el campo periodo convertido a datetime\n",
    "#   Este archivo lo uso en Orange y nec que periodo tenga formato fecha\n",
    "sell_780_all_toDatetime_toCsv = sell_780_all.copy()\n",
    "sell_780_all_toDatetime_toCsv['periodo'] = pd.to_datetime(sell_780_all_toDatetime_toCsv['periodo'], format='%Y%m')\n",
    "sell_780_all_toDatetime_toCsv.to_csv('../66_Datos/sell-z-780-all-prodId-toDatetime.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sell_780_all.loc[(sell_780_all['product_id'] == 20001) & (sell_780_all['periodo'] == 201701)].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sell-z-39-all-prodId + sell-z-39-groupBy-Period-ProdId\n",
    "\n",
    "Version con 39 productos que representan el 5% para pruebas rapidas\n",
    "\n",
    "\n",
    "\n",
    "sell-z-39-groupBy-Period-ProdId.csv es una version reducida de los datos de ventas (con el 5% de productos a predecir= y contiene los datos de ventas agrupados por 'periodo' y 'product_id'. Es decir, para un producto que tuvo multiples ventas en un periodo tengo agrupadas las ventas para ese periodo.\n",
    "\n",
    "Ej. \n",
    "| Periodo | product_id | tn   |\n",
    "|---------|------------|------|\n",
    "| 201801 | 20001      | 1520 |\n",
    "| 201801 | 20055      | 3360 |\n",
    "| 201802 | 20011      | 1320 |\n",
    "| 201802 | 20021      | 1220 |\n",
    "| 201802 | 20031      | 120 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de ventas y productos a predecir\n",
    "ventas_file_path = '../66_Datos/sell-in.txt'\n",
    "productos_file_path = '../66_Datos/productos_a_predecir-39.txt'\n",
    "\n",
    "df_ventas = pd.read_csv(ventas_file_path, sep='\\t')\n",
    "df_productos = pd.read_csv(productos_file_path, sep='\\t')\n",
    "\n",
    "# Filtrar las ventas para que solo incluya los productos a predecir\n",
    "productos_a_predecir = df_productos['product_id'].unique()\n",
    "sell_39_all = df_ventas[df_ventas['product_id'].isin(productos_a_predecir)]\n",
    "\n",
    "\n",
    "sell_39_groupBy_Periodo_ProdId = sell_39_all.groupby(['periodo', 'product_id'], as_index=False).sum()\n",
    "\n",
    "# Opcional: Guardar el DataFrame filtrado en un archivo CSV\n",
    "sell_39_all.to_csv('../66_Datos/sell-z-39-all-prodId.csv', index=False)\n",
    "sell_39_groupBy_Periodo_ProdId.to_csv('../66_Datos/sell-z-39-groupBy-Period-ProdId.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA con Ydata-Profiling \n",
    "\n",
    "Este código carga un conjunto de datos de ventas desde un archivo CSV y utiliza la herramienta `ydata_profiling` para generar un informe detallado de perfilado de datos, que se guarda en un archivo HTML para su posterior análisis y visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "ventas_LTSM_path = './66_Datos/sell-z-780-all-LTSM.csv'\n",
    "df_ventas = pd.read_csv(ventas_LTSM_path)\n",
    "\n",
    "profile = ProfileReport(df_ventas, tsmode=False, title=\"Profiling Report\")\n",
    "\n",
    "profile.to_file(\"./EDA_ydataProfiling/eda-yprof.html\")\n",
    "\n",
    "profile = ProfileReport(df_ventas, tsmode=True, title=\"Profiling Report\")\n",
    "\n",
    "profile.to_file(\"./EDA_ydataProfiling/eda-yprof-ts.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc Temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(range(2021, 2030))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Crear un DataFrame con fechas desde 2021 hasta 2039\n",
    "f2 = pd.DataFrame({\n",
    "    'ds': pd.to_datetime([str(x) for x in range(2021, 2040)], format='%Y'),\n",
    "    'y': np.nan\n",
    "})\n",
    "\n",
    "print(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 2021 a 2090\n",
    "futuro = pd.DataFrame({\n",
    "    'ds': pd.to_datetime([x for x in range(2021, 2025)], format='%Y'),\n",
    "    'y': np.nan\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime([x for x in range(2021, 2025)], format='%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ventas To Postgres\n",
    "\n",
    "Generar archivo sell-in-to-Postgres con:\n",
    " 1. Todas las ventas registradas en sell-in.txt\n",
    " 2. Agregar columna `en780` indicando con 1 aquellas filas que contienen product_id perteneciente al listado de 780 productos a predecir (productos_a_predecir.csv)\n",
    " 3. Merge de las ventas en sell-in.txt con la descripcion de productos en tb_productos_descripcion.txt (join by product_id)\n",
    " 4. Agregar columna `cant_periodos`, y en cada celda agregar la cantidad de meses que tuvo ventas el product_id de la fila. Por ejemplo si el product_id 20001 tuvo ventas durante los 36 meses, en la columna cant_periodos debe ir 36. \n",
    " 5. Exportar a csv  sell-in-all-to-postgres.csv\n",
    "\n",
    "\n",
    "Como identificar productos vendidos durante los 36 meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los datos de ventas y productos a predecir\n",
    "ventas_file_path = '../66_Datos/sell-in.txt'\n",
    "productos_file_path = '../66_Datos/productos_a_predecir.txt'\n",
    "productos_descripcion = '../66_Datos/tb_productos_descripcion.txt'\n",
    "\n",
    "\n",
    "# tb_productos_descripcion.txt es la ultima tabla (2024/06/15) que presento con las caracteristicas y categoria de cada producto\n",
    "df_productos_descripcion = pd.read_csv(productos_descripcion, sep='\\t')\n",
    "df_productos_descripcion = df_productos_descripcion[['product_id', 'cat1', 'cat2', 'cat3', 'brand', 'sku_size', 'descripcion']]\n",
    "df_productos_descripcion = df_productos_descripcion.reset_index(drop=True)\n",
    "# Nota: no encuentro duplicados en productos_descripcion. De todas formas armo la sentencia de eliminarlos ya que pueden aparecer\n",
    "#       futuras versiones del archivo\n",
    "# Eliminar los duplicados en la columna 'product_id', manteniendo solo la primera aparición\n",
    "df_productos_descripcion.drop_duplicates(subset='product_id', keep='first', inplace=True)\n",
    "\n",
    "df_ventas_orig = pd.read_csv(ventas_file_path, sep='\\t')\n",
    "\n",
    "df_ventas = pd.merge(df_ventas_orig, df_productos_descripcion, on=['product_id'], how='left')\n",
    "\n",
    "# Cargamos los productos a predecir (780)\n",
    "df_productos = pd.read_csv(productos_file_path, sep='\\t')\n",
    "\n",
    "\n",
    "# Filtrar las ventas para que solo incluya los productos a predecir. Va .unique() \n",
    "productos_a_predecir = df_productos['product_id'].unique()\n",
    "\n",
    "# Agregar la columna \"en780\" a df_ventas\n",
    "df_ventas['en780'] = df_ventas['product_id'].apply(lambda x: 1 if x in productos_a_predecir else 0)\n",
    "\n",
    "# Agregar columna `cant_periodos`\n",
    "# Primero convertimos la columna 'periodo' en datetime si no está en ese formato\n",
    "df_ventas['periodo'] = pd.to_datetime(df_ventas['periodo'], format='%Y%m')\n",
    "\n",
    "# Agrupamos por 'product_id' y contamos los periodos únicos\n",
    "cant_periodos = df_ventas.groupby('product_id')['periodo'].nunique()\n",
    "\n",
    "# Agregamos esta información al DataFrame original\n",
    "df_ventas = df_ventas.merge(cant_periodos.rename('cant_periodos'), on='product_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores nulos df_productos_descripcion y df_ventas\n",
    "\n",
    "Resulta que en la tabla productos_descripcion.txt hay un listado de 45 productos vendidos durante los 36 peridos que no figuran.\n",
    " - Voy a completar esos nulos tanto en la tabla que llevo a postgres sell_all (sell-in-all-to-postgres.csv) como productos2 (tb_productos_descripcion_ac.csv)\n",
    " - Estoy completando las variables cat1, cat2, cat3, brand y descripción con el string 'sinCatego'\n",
    " - Estoy completando la variable sku_size con 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay valores nulos en cat1, cat2, cat3, brand, sku_size, descripcion\n",
    "nulos_cat1 = df_ventas['cat1'].isnull().sum()\n",
    "nulos_cat2 = df_ventas['cat2'].isnull().sum()\n",
    "nulos_cat3 = df_ventas['cat3'].isnull().sum()\n",
    "nulos_brand = df_ventas['brand'].isnull().sum()\n",
    "nulos_sku_size = df_ventas['sku_size'].isnull().sum()\n",
    "nulos_descripcion = df_ventas['descripcion'].isnull().sum()\n",
    "\n",
    "print(f'Valores nulos en cat1: {nulos_cat1}')\n",
    "print(f'Valores nulos en cat2: {nulos_cat2}')\n",
    "print(f'Valores nulos en cat3: {nulos_cat3}')\n",
    "print(f'Valores nulos en brand: {nulos_brand}')\n",
    "print(f'Valores nulos en sku_size: {nulos_sku_size}')\n",
    "print(f'Valores nulos en descripcion: {nulos_descripcion}')\n",
    "\n",
    "\n",
    "# Reemplazar valores nulos en columnas tipo object con \"sinCatego\"\n",
    "cols_object = ['cat1', 'cat2', 'cat3', 'brand', 'descripcion']\n",
    "for col in cols_object:\n",
    "    df_ventas[col].fillna('sinCatego', inplace=True)\n",
    "\n",
    "# Reemplazar valores nulos en columnas numéricas con 0\n",
    "cols_numeric = ['sku_size']\n",
    "for col in cols_numeric:\n",
    "    df_ventas[col].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# Verificar si hay valores nulos en cat1, cat2, cat3, brand, sku_size, descripcion\n",
    "nulos_cat1 = df_ventas['cat1'].isnull().sum()\n",
    "nulos_cat2 = df_ventas['cat2'].isnull().sum()\n",
    "nulos_cat3 = df_ventas['cat3'].isnull().sum()\n",
    "nulos_brand = df_ventas['brand'].isnull().sum()\n",
    "nulos_sku_size = df_ventas['sku_size'].isnull().sum()\n",
    "nulos_descripcion = df_ventas['descripcion'].isnull().sum()\n",
    "\n",
    "print(f'Valores nulos en cat1: {nulos_cat1}')\n",
    "print(f'Valores nulos en cat2: {nulos_cat2}')\n",
    "print(f'Valores nulos en cat3: {nulos_cat3}')\n",
    "print(f'Valores nulos en brand: {nulos_brand}')\n",
    "print(f'Valores nulos en sku_size: {nulos_sku_size}')\n",
    "print(f'Valores nulos en descripcion: {nulos_descripcion}')\n",
    "\n",
    "\n",
    "print('--------------------------------------')\n",
    "print('--------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "# Filtrar los productos en df_ventas donde cat1 es 'sinCatego'\n",
    "productos_sin_categoria = df_ventas[df_ventas['cat1'] == 'sinCatego']['product_id'].unique()\n",
    "\n",
    "# Encontrar los product_id que faltan en df_productos_descripcion\n",
    "productos_descripcion_ids = df_productos_descripcion['product_id'].unique()\n",
    "productos_faltantes = set(productos_sin_categoria) - set(productos_descripcion_ids)\n",
    "\n",
    "# Mostrar los product_id que faltan en df_productos_descripcion\n",
    "print('Product IDs que faltan en df_productos_descripcion:')\n",
    "print(productos_faltantes)\n",
    "\n",
    "\n",
    "print('--------------------------------------')\n",
    "print('--------------------------------------')\n",
    "\n",
    "\n",
    "# Crear un DataFrame con los productos faltantes\n",
    "productos_faltantes_df = pd.DataFrame({\n",
    "    'product_id': list(productos_faltantes),\n",
    "    'cat1': 'sinCatego',\n",
    "    'cat2': 'sinCatego',\n",
    "    'cat3': 'sinCatego',\n",
    "    'brand': 'sinCatego',\n",
    "    'sku_size': 0,\n",
    "    'descripcion': 'sinCatego'\n",
    "})\n",
    "\n",
    "# Concatenar el DataFrame de productos faltantes con df_productos_descripcion\n",
    "df_productos_descripcion_completo = pd.concat([df_productos_descripcion, productos_faltantes_df], ignore_index=True)\n",
    "\n",
    "# Verificar el resultado\n",
    "print(df_productos_descripcion_completo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Guardar el DataFrame filtrado en un archivo CSV\n",
    "#df['ds'] = pd.to_datetime(df['periodo'], format='%Y%m')\n",
    "df_ventas_toCsv = df_ventas\n",
    "#df_ventas_toCsv['periodo'] = pd.to_datetime(df_ventas_toCsv['periodo'], format='%Y%m')\n",
    "\n",
    "# Genero el csv para crear en pg z_l3.sell_all\n",
    "df_ventas_toCsv.to_csv('../66_Datos/sell-in-all-to-postgres.csv', index=False)\n",
    "\n",
    "\n",
    "# Genero el csv para crear en pg z_l3.productos2\n",
    "df_productos_descripcion_completo.to_csv('../66_Datos/tb_productos_descripcion_ac.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ventas.loc[df_ventas.product_id == 20808].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
